{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a4e006-bdda-4dbf-9b54-8d574bdb84c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e77400-175f-445a-8dbd-b53cbc2b63ce",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de451bb-cb53-421a-9b01-ad5061b510b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "R-squared is a goodness-of-fit measure for linear regression models. It indicates the percentage of the variance in the dependent\n",
    "variable that the independent variables explain collectively. R-squared measures the strength of the relationship between your model\n",
    "and the dependent variable on a convenient 0 – 100% scale1.\n",
    "\n",
    "The formula for calculating R-squared is:\n",
    "\n",
    "R-squared = 1 - (SSres / SStot)\n",
    "\n",
    "where SSres is the sum of squared residuals and SStot is the total sum of squares.\n",
    "\n",
    "R-squared ranges between 0 and 1. An R-squared value of 1 indicates that all variations in the dependent variable are explained by\n",
    "the independent variables. An R-squared value of 0 indicates that none of the variations in the dependent variable are explained by\n",
    "the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444e2205-d63b-48cf-9cef-f934c849b55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5954a50e-db3b-4042-987c-3ce282c41b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa32a39-8c9d-4108-8642-f91784bef68d",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd7635d-6d38-455d-b25a-75a689326fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "R-squared is a statistical measure that represents the proportion of the variance for a dependent variable that’s explained by an \n",
    "independent variable or variables in a regression model. It is also known as the coefficient of determination. Adjusted R-squared \n",
    "is a modified version of R-squared that has been adjusted for the number of predictors in the model. It penalizes the addition of\n",
    "irrelevant predictors and can be more accurate than R-squared when testing different independent variables against a dependent \n",
    "variable.\n",
    "\n",
    "In other words, adjusted R-squared is used to determine how well a multiple regression model fits the data being analyzed. \n",
    "It is calculated as 1 - [(1 - R-squared) * ((n - 1) / (n - k - 1))], where n is the number of observations and k is the number of\n",
    "independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65b1dc-f860-455e-8033-138bb5174e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c272c2-ab98-4041-b7f8-4f7ab69b96ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5babd9-3f8f-48bc-800a-5cde1b606474",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b17b4f-4293-4f4c-a079-1fd187a654f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model.\n",
    "It determines the extent of the variance of the dependent variable, which the independent variable can explain. The adjusted\n",
    "R-squared increases when a new term improves the model more than would be expected by chance, and decreases when a predictor\n",
    "improves the model by less than expected. It can be used to compare the fit of regression models with different numbers of \n",
    "predictor variables.\n",
    "\n",
    "In general, adjusted R-squared is more appropriate than R-squared when comparing models with different numbers of predictors. \n",
    "This is because R-squared increases as more predictors are added to the regression model, even if they do not improve the model’s \n",
    "fit. Adjusted R-squared takes into account the number of predictors in the model and adjusts for this effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a524c93c-396d-4bc2-a3f3-3e514a0ecd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d998ecb2-0153-4e78-bf7b-77ab9867033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73e4dc5-59ef-493b-ac59-f72b27c17de7",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c05ea8d-1a14-489c-8a15-05efb9344eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "In regression analysis, **RMSE** (Root Mean Squared Error), **MSE** (Mean Squared Error), and **MAE** (Mean Absolute Error) are\n",
    "metrics used to evaluate the performance of regression models. \n",
    "\n",
    "**MAE** is the average of the absolute differences between the predicted and actual values. It measures the average magnitude of the\n",
    "errors in a set of predictions, without considering their direction. \n",
    "\n",
    "**MSE** is the average of the squared differences between the predicted and actual values. It measures the average squared difference\n",
    "between the estimated values and what is estimated. \n",
    "\n",
    "**RMSE** is the square root of MSE. It is used to measure how much error there is between two datasets. RMSE is widely used in\n",
    "regression analysis because it penalizes large errors more than MAE does.\n",
    "\n",
    "The formulas for these metrics are as follows:\n",
    "\n",
    "- MAE = (1/n) * Σ|yi - xi|\n",
    "- MSE = (1/n) * Σ(yi - xi)^2\n",
    "- RMSE = sqrt(MSE)\n",
    "\n",
    "where n is the number of observations, yi is the predicted value, and xi is the actual value.\n",
    "\n",
    "These metrics are used to evaluate how well a regression model fits a dataset. A lower value for these metrics indicates that a \n",
    "model has better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510b5c8-b4ed-46f5-8cdd-b58fce312010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d9d6a-2d50-47e9-8c26-fe492dacf47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f5f07e-b913-4e38-b6aa-0492591df97c",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae795c-d170-45c3-b88b-4f8175f09245",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) are all evaluation metrics used in \n",
    "regression analysis.\n",
    "\n",
    "The MAE is the average of the absolute differences between predicted and actual values. It is less sensitive to outliers than MSE \n",
    "and RMSE. However, it does not penalize large errors as much as MSE and RMSE do.\n",
    "\n",
    "The MSE is the average of the squared differences between predicted and actual values. It is more sensitive to outliers than MAE \n",
    "but less sensitive than RMSE. It penalizes large errors more than MAE does.\n",
    "\n",
    "The RMSE is the square root of the average of the squared differences between predicted and actual values. It is more sensitive to\n",
    "outliers than MAE but less sensitive than MSE. It penalizes large errors more than both MAE and MSE do.\n",
    "\n",
    "In summary, MAE is a good metric when you want to minimize the impact of outliers on your model’s performance. MSE is a good metric \n",
    "when you want to penalize large errors more heavily. RMSE is a good metric when you want to penalize large errors more heavily but \n",
    "also want to express the error in the same units as your target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4b7c76-5c1d-4266-b518-962393bfe147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23fcc1-5fc8-4ad7-9394-23899c493236",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dcfea9-793a-4c75-b1fa-0a63c74538d1",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383f6392-c72a-47e1-8f04-51cf31721a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso and Ridge are two forms of regularization that add a penalty term to your loss function to help deal with overfitting. \n",
    "Ridge regularization, also called an L2 penalty, is going to square your coefficients. This shrinks the coefficients towards zero. \n",
    "Lasso regularization, or an L1 penalty, is going to take the absolute value of your coefficients. This encourages some of the \n",
    "coefficients to be exactly zero.\n",
    "\n",
    "The difference between Lasso and Ridge regularization is that Lasso can lead to zero coefficients while Ridge regression does not \n",
    "force any coefficients to be zero. When we have more number of features than observations, Ridge regression is a better choice.\n",
    "When we have fewer observations than features, Lasso is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b83e438-211b-456f-af7f-1913a68a4299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98962ca7-2a80-4a99-afe6-1af30fc5e634",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ef38ce-711d-4f54-bf45-0a74024c4343",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97996210-b852-4fd2-8ebc-989e14bf9322",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularized linear models are a type of linear regression model that adds a penalty term to the loss function. \n",
    "This penalty term is used to control the complexity of the model and avoid overfitting. Overfitting occurs when a model is too \n",
    "complex and fits the training data too closely, resulting in poor performance on new data.\n",
    "\n",
    "There are several types of regularization techniques, including L1 regularization (Lasso), L2 regularization (Ridge), and Elastic\n",
    "Net regularization. L1 regularization adds an absolute value of the coefficients to the loss function, while L2 regularization adds\n",
    "a squared value of the coefficients. Elastic Net regularization is a combination of both L1 and L2 regularization.\n",
    "\n",
    "For example, suppose you have a dataset with many features, but only a few of them are important for predicting the target variable.\n",
    "A regularized linear model can be used to select only the important features by shrinking the coefficients of the unimportant\n",
    "features to zero. This helps to prevent overfitting and improve the performance of the model on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362d8f8b-54ad-4b66-9624-96aa6b66eddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a3bf2-7bcf-47a4-a0bf-448e7e2c9ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb62651-a1d0-48f7-b39e-5bf697dbfdc9",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc90a9-7126-47ac-9f22-688bdc424ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularized linear models are a popular choice for regression analysis because they can help reduce overfitting and improve the\n",
    "generalization of the model. However, they may not always be the best choice for regression analysis. Here are some limitations of\n",
    "regularized linear models:\n",
    "\n",
    "Feature selection: Regularized linear models can be used for feature selection, but they may not always select the best features.\n",
    "                   In some cases, other feature selection methods may be more effective.\n",
    "\n",
    "Interpretability: Regularized linear models can be difficult to interpret because the coefficients are shrunk towards zero.\n",
    "                  This can make it difficult to understand the relationship between the features and the target variable.\n",
    "\n",
    "Non-linear relationships: Regularized linear models assume that the relationship between the features and the target variable is \n",
    "                          linear. If there is a non-linear relationship, then a different type of model may be more appropriate.\n",
    "\n",
    "Outliers: Regularized linear models can be sensitive to outliers. If there are outliers in the data, then a different type of model \n",
    "           may be more appropriate.\n",
    "\n",
    "Computational complexity: Regularized linear models can be computationally expensive to train. If you have a large dataset or \n",
    "                          many features, then a different type of model may be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f325fb-435c-4946-ae53-a834030b2a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c3804-3c49-49c6-9b1d-8fda4dfc7c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a77aba-6afb-49c1-884f-2b1dcd807461",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac38e69-6b3e-4d8d-ad11-a5f2b4305928",
   "metadata": {},
   "outputs": [],
   "source": [
    "Both RMSE and MAE are metrics that measure the average distance of the error between the ground truth and the predicted value. \n",
    "RMSE is more sensitive to outliers, as it squares the errors before taking the average. MAE is more robust to outliers, as it takes \n",
    "the absolute value of the errors. Lower values of both metrics indicate better performance.\n",
    "\n",
    "In  case, Model A has an RMSE of 10 while Model B has an MAE of 8. Since both models have different evaluation metrics, \n",
    "it is not possible to compare them directly. However, if we assume that both models have similar error distributions, then we can \n",
    "say that Model B is better than Model A because it has a lower error value.\n",
    "\n",
    "There are some limitations to using these metrics. For example, they do not provide any information about the direction or sign of\n",
    "the error. They only measure the magnitude of the error. Also, they do not take into account any differences between overestimation \n",
    "and underestimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e249169a-c298-49cf-b789-9ab14bcf7cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4152098d-73b9-49ec-a01f-64ed6a760c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e0b199-01a7-4028-a191-309bba6878f3",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc07ff3d-29dc-43af-982d-833c5dd0ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "Both Ridge and Lasso regression are regularization techniques that help prevent overfitting in linear regression models.\n",
    "Ridge regression adds an L2 penalty term to the loss function, which shrinks the coefficients towards zero. Lasso regression adds \n",
    "an L1 penalty term to the loss function, which encourages some of the coefficients to be exactly zero. \n",
    "\n",
    "In your case, Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization \n",
    "with a regularization parameter of 0.5. The choice of regularization method depends on the data and the problem at hand.\n",
    "Ridge regression is generally used when all the features are important and you want to avoid overfitting by shrinking the \n",
    "coefficients towards zero. Lasso regression is generally used when you have many features with high correlation and you need to\n",
    "take away the useless features by encouraging some of the coefficients to be exactly zero.\n",
    "\n",
    "In terms of performance, it is difficult to say which model is better without knowing more about your data and problem. However,\n",
    "if you have many features with high correlation and you need to take away the useless features then Lasso is a better solution. \n",
    "If all the features are important then Ridge is a better solution.\n",
    "\n",
    "There are trade-offs between Ridge and Lasso regularization methods. Ridge regression tends to perform better than Lasso when there \n",
    "are many predictors with small or medium-sized effects. However, if there are many predictors with large effects, then Lasso may\n",
    "perform better. Another trade-off is that Ridge regression will shrink all coefficients towards zero but will not set any of them\n",
    "exactly to zero. On the other hand, Lasso regression can set some coefficients exactly to zero. This can be useful for feature \n",
    "selection but can also lead to a less interpretable model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
